{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c18455-71ac-4bc7-8568-5f77c4b60ac1",
   "metadata": {},
   "source": [
    "# Segment 1 Lab 1\n",
    "\n",
    "# STRAIGHT TO ACTION!\n",
    "\n",
    "Welcome to our first Lab where we will see rapid, satisfying results!\n",
    "\n",
    "I will leave with you to try out leading LLMs through their Chat Interfaces\n",
    "\n",
    "Together, we will call them using their APIs\n",
    "\n",
    "Please see the README for instructions on setting this up and getting your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810279d-0b70-457d-8713-d9023a6650bf",
   "metadata": {},
   "source": [
    "# If this is your first time in a Jupyter Notebook..\n",
    "\n",
    "Welcome to the world of Data Science experimentation. Warning: Jupyter Notebooks are very addictive and you may find it hard to go back to IDEs afterwards!!\n",
    "\n",
    "Simply click in each cell with code and press `Shift + Enter` to execute the code and print the results.\n",
    "\n",
    "There's a notebook called \"Guide to Jupyter\" in the parent directory that will give you a handy tutorial on all things Jupyter Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24228179-fe9e-4098-8669-6898459eaa76",
   "metadata": {},
   "source": [
    "## FIRST: Calling Frontier Models through APIs\n",
    "\n",
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables.\n",
    "\n",
    "Create a file called `.env` in this project root directory, and set your keys there:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3506e-b46f-44f0-ba9b-6b002835d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae0a99-0235-4256-9874-1b5e718b6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66139a-6ae1-41b6-85b1-8d71de1348be",
   "metadata": {},
   "source": [
    "## Connecting to Python Client libraries\n",
    "\n",
    "We call Cloud APIs by making REST calls to an HTTP endpoint, passing in our keys.\n",
    "\n",
    "For convenience, the labs like OpenAI have provided lightweight python client libraries that make the HTTP calls for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c6483-a9a6-4e3b-b361-e7a8c7f912e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrappers around calls to REST endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Anthropic, Gemini, DeepSeek, Groq, Grok we can use the OpenAI python client\n",
    "# Because they all have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "# And you can even use the same approach for Ollama locally\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "ollama = OpenAI(api_key='ollama', base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584134a-68c6-4677-b4b0-1a7c1c93aeb4",
   "metadata": {},
   "source": [
    "## Asking LLMs a hard nuanced question to compare their capabilities\n",
    "\n",
    "Let's compare models using a challenging question that requires them to show creativity and thoughtfulness.\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A **system prompt** that gives overall context for the role the LLM is playing\n",
    "- A **user prompt** that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic.\n",
    "\n",
    "### The standard format of messages with an LLM, first used by OpenAI in its API and now adopted more widely\n",
    "\n",
    "Conversations use this format:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### The standard API call that has become ubiquitous: _\"Chat Completions\"_\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(...)\n",
    "reply = response.choices[0].message.content\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbcc5b-8517-4a3e-9ca1-e860a072bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\"\n",
    "user_prompt = \"How would you describe the color blue to someone who has never been able to see, in 1 sentence.\"\n",
    "\n",
    "# blue = ...\n",
    "\n",
    "blue = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14234bb0-40c5-4e64-aba5-852b392f780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4c88c-e809-4d89-9184-f2032c02a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an easy puzzle\n",
    "\n",
    "easy_user_prompt = \"I toss 2 coins. One of them is heads. What's the chances the other is tails? Answer with the probability only\"\n",
    "easy = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": easy_user_prompt}\n",
    "  ]\n",
    "easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b64a5-5e72-4e04-8828-5f091bd50560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a much harder question..\n",
    "\n",
    "puzzle_user_prompt = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through? Answer only with the distance.\n",
    "\"\"\"\n",
    "\n",
    "puzzle = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": puzzle_user_prompt}\n",
    "  ]\n",
    "puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945bc543-2688-4007-8288-b85370c2bd4a",
   "metadata": {},
   "source": [
    "## To recap\n",
    "\n",
    "We've set up three messages to test out models using their API:\n",
    "\n",
    "`blue` asks to describe the color blue  \n",
    "`easy` is a sneaky probability question  \n",
    "`puzzle` is a tricky brainteaser that I failed at!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70354078-8a5a-405e-88f0-0a9878bcc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-nano\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4.1-nano\", messages=easy)\n",
    "response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cd59d-3efe-41e7-b149-ce56b9697a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-5-nano\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=easy, reasoning_effort=\"minimal\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbb3fe-6c9e-4f1a-ae16-c20445fc5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.0 Sonnet\n",
    "\n",
    "response = anthropic.chat.completions.create(\n",
    "    model='claude-sonnet-4-5',\n",
    "    messages=blue,\n",
    "    temperature=1.9\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543cc49-fe01-4a6c-af4e-c576f4bbc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini using its OpenAI endpoint\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model='gemini-2.5-flash',\n",
    "    messages=blue,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5390985-40fe-4799-bb09-65d1db056fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseek-V3 - the full 671B params\n",
    "\n",
    "response = deepseek.chat.completions.create(\n",
    "    model='deepseek-chat',\n",
    "    messages=blue\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c850bcc-a43d-4f58-965b-59309e8e9b0d",
   "metadata": {},
   "source": [
    "## Let's test the most powerful models on the planet\n",
    "\n",
    "_And no response after 30s is considered a fail_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f50f24b-33f5-40dd-abe9-df0e27ad2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func_timeout import func_set_timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53776f-5fde-4ab3-9fca-a001bd0718c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_set_timeout(30)\n",
    "def solve(client, model, task=puzzle, effort=None):\n",
    "    stream = client.chat.completions.create(model=model, messages=task, reasoning_effort=effort, stream=True)\n",
    "    for chunk in stream:\n",
    "        print(chunk.choices[0].delta.content or '', end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35edf934-5292-4b80-b19c-a58bc07cc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-5\n",
    "\n",
    "solve(openai, \"gpt-5\", puzzle, \"low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0bd6c-779b-410a-869c-e236dc9670da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grok-4 - times out with puzzle!\n",
    "\n",
    "solve(grok, \"grok-4\", blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9b009-022a-474c-b969-68b44d846bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 2.5 Pro also times out\n",
    "\n",
    "solve(gemini, \"gemini-2.5-pro\", blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2600f-2e77-412c-82b6-78ba4bff23ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.5\n",
    "\n",
    "solve(anthropic, \"claude-sonnet-4-5\", puzzle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec4ff3-ef56-4c00-a523-b13cd9b284c6",
   "metadata": {},
   "source": [
    "## Looking at Open-Source models running locally via Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1dd467-914c-41b3-9f1e-a5bc87b96f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2:1b\n",
    "!ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc98da-149f-4d80-99c9-d2df4c612a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama 3.2 with 1B params\n",
    "\n",
    "solve(ollama, \"llama3.2:1b\", blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5267555-6a83-4ad8-b883-c2e699f0d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The brand new OSS 20B model running locally!\n",
    "\n",
    "solve(ollama, \"gpt-oss:20b\", blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7a0e3-785b-403d-91a1-d28ea8297943",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08b981-f27e-4e50-aaa7-6ab1c0e09f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-nano and Claude-3-5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-nano\"\n",
    "claude_model = \"claude-haiku-4-5\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557a7c8-b574-4394-a6ca-97d49a45d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    response = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b352f82-a94e-43b5-a4c7-71c6eeb5d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(call_gpt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9cc43-e9b7-4179-b318-1e60998665bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(\n",
    "        model=claude_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23ebf0-2816-4953-94ec-a96b931179d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81c5f7-1cb1-439e-b67a-46295ee7c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16d1f1-ea9c-45c2-9228-7f63d1b00a24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c88f8-9b16-475e-9f98-a3c7a2a1caed",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "This was an entertaining exercise!\n",
    "\n",
    "At the same time, it hopefully gave you some perspective on:\n",
    "- The use of system prompts to set tone and character\n",
    "- The way that the entire conversation history is passed in to each API call, giving the illusion that LLMs have memory of the chat so far\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Try different characters; try swapping Claude with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2b8e3-182b-430b-9c89-d5e89f32a286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

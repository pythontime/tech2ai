{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d67b35-9316-4271-bcc7-7e1f7795d167",
   "metadata": {},
   "source": [
    "# Segment 2 Lab 1\n",
    "\n",
    "Linear regression, generalization and overfitting\n",
    "\n",
    "## Disclaimer again!\n",
    "\n",
    "I will go through this quite fast - if you're new to ML, use this to gain intuition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511e864-4653-4d5a-8026-470bab590399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from incredibly common packages: numpy, matplotlib, sci-kit learn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de1b7d7-18ba-4d86-8c1a-460c1d4508ee",
   "metadata": {},
   "source": [
    "# To set up our problem\n",
    "\n",
    "## We have 100 Home Appliances.\n",
    "\n",
    "Each of them have a **price** and a **height**.\n",
    "\n",
    "Unknown to us - all the home appliances have a price that is (strangely) closely related to its height!\n",
    "\n",
    "The price is actually closely given by:\n",
    "$$\n",
    "price = 200 + 100 \\cdot height - 40 \\cdot height^2 + 4 \\cdot height^3\n",
    "$$\n",
    "\n",
    "This means that if we were to create 3 features:\n",
    "1. height\n",
    "2. height squared\n",
    "3. height cubed\n",
    "\n",
    "Then a linear regression model would be able to predict the price closely.\n",
    "\n",
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d6559-cdfa-4d47-b7b8-c32f44ec74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for Home Appliances: 100 points with prices that roughly follow a cubic curve\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "heights = np.random.uniform(0, 10, n_samples)\n",
    "noise = np.random.normal(0, 50, n_samples)\n",
    "prices = 200 + 100*heights - 40*heights**2 + 4*heights**3 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bec48c-3f3f-4f4c-967e-eb8d42a939b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the generated data\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(heights, prices, alpha=0.5)\n",
    "plt.title('Home Appliance Price vs Height')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a74594-610f-412d-a940-34492902ca3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Every numpy array has a shape that represents the dimensions of the array\n",
    "# Our weights are currently a one dimensional list of floats:\n",
    "\n",
    "print(heights.shape)\n",
    "print(heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c31f2-8b2e-48f0-ae48-92a052ce8e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We want to turn this into a 2D matrix, with a row for each of our 100 datapoints\n",
    "# And columns for each feature (there is only 1)\n",
    "\n",
    "X = heights.reshape(-1, 1)\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f26512-ba36-480a-9d57-f499c9dc19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this utility method from sk-learn to split up our data into train and test:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, prices, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a389479-1fa0-41d2-aa90-a91ea760aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Linear Regression is simply a couple of lines of code - predict price based only on height:\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae9ffb-e840-4ab7-af02-48762f8b58b3",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "Now we have a trained model and we can make predictions. We will run the model on our test dataset.\n",
    "\n",
    "We'll calculate 2 key metrics: Mean Squared Error and R squared.\n",
    "\n",
    "**Mean Squared Error** as it sounds, the average of the square of the difference between y and y hat\n",
    "\n",
    "**R squared** measures how well the predictions fit the truth;  \n",
    "0 means that the model explains none of the variability and is no better than always guessing the mean value   \n",
    "100% means it fully explains it  \n",
    "A negative number suggests that the model is worse than just guessing the average!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae906ce5-bdc4-40ee-90ca-52ebe1cced98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on test dataset using model.predict\n",
    "\n",
    "y_hat_test = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_hat_test)\n",
    "r2 = r2_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9c272-3a17-43c1-a0f7-2b41210ddac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the results\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, alpha=0.5)\n",
    "plt.plot(X_test, y_hat_test, color='red', linewidth=2)\n",
    "plt.title(f'Linear Regression with 1 feature (height) - performance on unseen Test data\\nMSE: {mse:,.0f}\\nR squared: {r2*100:.0f}%')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1d98a-1f61-43a0-b648-01054d4d1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's add a SECOND feature to our input data, in addition to the height\n",
    "# Let's add a feature which is the SQUARE of the height\n",
    "\n",
    "X2 = np.column_stack((heights, heights**2))\n",
    "print(X2.shape)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdfe54-76f7-4828-847c-ae000455d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test and run Linear Regression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, prices, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_hat_test = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_hat_test)\n",
    "r2 = r2_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb89bc6-36c2-413a-ad37-beca8df8daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort X_test for smooth curve plotting\n",
    "\n",
    "X_test_sorted = X_test[X_test[:, 0].argsort()]\n",
    "y_hat_test_sorted = model.predict(X_test_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934295a-87ba-4c5d-a035-d295509bc733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test[:, 0], y_test, alpha=0.5)\n",
    "plt.plot(X_test_sorted[:, 0], y_hat_test_sorted, color='red', linewidth=2)\n",
    "plt.title(f'Linear Regression - performance on unseen Test data\\nMSE: {mse:,.0f}\\nR squared: {r2*100:.0f}%')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd70f62-8216-41f8-86cd-9fb36ef37380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Polynomial Regression (Degree 3)\n",
    "\n",
    "X3 = np.column_stack((heights, heights**2, heights**3))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3, prices, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_hat_test = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_hat_test)\n",
    "r2 = r2_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574b17b-a44d-4fda-b0f8-9ba67e2308e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort X_test for smooth curve plotting\n",
    "\n",
    "X_test_sorted = X_test[X_test[:, 0].argsort()]\n",
    "y_hat_test_sorted = model.predict(X_test_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f36d60-1467-4f42-ba46-c8359069cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test[:, 0], y_test, alpha=0.5)\n",
    "plt.plot(X_test_sorted[:, 0], y_hat_test_sorted, color='red', linewidth=2)\n",
    "plt.title(f'Linear Regression - performance on unseen Test data\\nMSE: {mse:,.0f}\\nR squared: {r2*100:.0f}%')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8afc386-b2ab-4695-b4c0-7b09cec5042b",
   "metadata": {},
   "source": [
    "Remember the true formula is: $price = 200 + 100 \\cdot height - 40 \\cdot height^2 + 4 \\cdot height^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62986b90-3411-4d0f-9127-041e4fe065f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4da12-5617-471b-9003-d0d9876803a0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "When we give our Linear Regression model three features:\n",
    "\n",
    "height  \n",
    "height squared  \n",
    "height cubed  \n",
    "\n",
    "It's able to fit and then it generalizes well, because the results look great on unseen data.\n",
    "\n",
    "And this makes sense because secretly, we generated data based on exactly those features.\n",
    "\n",
    "# AND NOW... to make this come to life\n",
    "\n",
    "We're going to make 376 features.\n",
    "\n",
    "So many features, that our model can PERFECTLY predict every item in the Training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27902566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overfit_features(\n",
    "    x,\n",
    "    n_poly=30,                 # x^1 ... x^n_poly\n",
    "    frac_feats=True,           # sqrt, cbrt, log, reciprocal\n",
    "    n_fourier=20,              # sin/cos pairs at different frequencies\n",
    "    n_hinges=20,               # ReLU hinges at many cutpoints\n",
    "    n_steps=10,                # hard indicator steps\n",
    "    n_rbf=30,                  # local Gaussian bumps\n",
    "    n_noise=60,                # pure noise columns\n",
    "    n_x_noise=20,              # x * noise interactions\n",
    "    n_dupes=10,                # duplicate some columns with tiny jitter\n",
    "    seed=0\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = np.asarray(x).reshape(-1)             \n",
    "    n = x.shape[0]\n",
    "\n",
    "    x_mean, x_std = x.mean(), (x.std() or 1.0)\n",
    "    z = (x - x_mean) / x_std\n",
    "\n",
    "    cols, names = [], []\n",
    "\n",
    "    # 1) Polynomials (mix scaled & unscaled to create collinearity)\n",
    "    for k in range(1, n_poly + 1):\n",
    "        cols.append(z**k); names.append(f\"z^{k}\")\n",
    "    for k in range(1, min(n_poly, 12) + 1):  # a few unscaled high-variance terms\n",
    "        cols.append(x**k); names.append(f\"x^{k}(raw)\")\n",
    "\n",
    "    # 2) Fractional/odd transforms\n",
    "    if frac_feats:\n",
    "        eps = 1e-6\n",
    "        cols += [np.sqrt(np.clip(x, 0, None)), np.cbrt(np.clip(x, -1e9, None))]\n",
    "        names += [\"sqrt(x)\", \"cbrt(x)\"]\n",
    "        cols += [np.log(np.abs(x) + 1.0), 1.0 / (np.abs(x) + 1.0 + eps)]\n",
    "        names += [\"log(|x|+1)\", \"1/(|x|+1)\"]\n",
    "\n",
    "    # 3) Fourier basis (varied frequencies)\n",
    "    # Use frequencies up to Nyquist-ish relative to the spread of x\n",
    "    x_span = max(x.max() - x.min(), 1.0)\n",
    "    base = np.pi / max(x_span, 1e-6)\n",
    "    for j in range(1, n_fourier + 1):\n",
    "        w = j * base\n",
    "        cols.append(np.sin(w * x)); names.append(f\"sin({j})\")\n",
    "        cols.append(np.cos(w * x)); names.append(f\"cos({j})\")\n",
    "\n",
    "    # 4) Hinges (ReLU) and Steps at random cutpoints across x\n",
    "    cuts = np.quantile(x, np.linspace(0.05, 0.95, n_hinges))\n",
    "    for c in cuts:\n",
    "        cols.append(np.maximum(0.0, x - c)); names.append(f\"(x-{c:.3g})_+\")\n",
    "    step_cuts = np.quantile(x, np.linspace(0.1, 0.9, n_steps))\n",
    "    for c in step_cuts:\n",
    "        cols.append((x > c).astype(float)); names.append(f\"1[x>{c:.3g}]\")\n",
    "\n",
    "    # 5) RBF bumps with small/medium bandwidths\n",
    "    centers = np.quantile(x, np.linspace(0.05, 0.95, n_rbf))\n",
    "    sigmas = np.linspace(0.15, 0.35, n_rbf) * max(x_std, 1e-6)\n",
    "    for c, s in zip(centers, sigmas):\n",
    "        cols.append(np.exp(-0.5 * ((x - c) / (s + 1e-12))**2))\n",
    "        names.append(f\"rbf(c={c:.3g},s={s:.3g})\")\n",
    "\n",
    "    # 6) Pure noise features\n",
    "    for j in range(n_noise):\n",
    "        eta = rng.standard_normal(n)\n",
    "        cols.append(eta); names.append(f\"noise[{j}]\")\n",
    "\n",
    "    # 7) Interactions of x with noise (sample-specific quirks)\n",
    "    for j in range(n_x_noise):\n",
    "        eta = rng.standard_normal(n)\n",
    "        cols.append(x * eta); names.append(f\"x*noise[{j}]\")\n",
    "\n",
    "    # 8) Duplicate a few columns with tiny jitter (ill-conditioning)\n",
    "    jitter_scale = 1e-4\n",
    "    pick = rng.integers(0, len(cols), size=n_dupes)\n",
    "    for idx in pick:\n",
    "        dup = cols[idx] + jitter_scale * rng.standard_normal(n)\n",
    "        cols.append(dup); names.append(f\"{names[idx]}_dup\")\n",
    "\n",
    "    X = np.column_stack(cols)\n",
    "    return X, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_overfit, feat_names = make_overfit_features(heights,\n",
    "                                      n_poly=40,\n",
    "                                      n_fourier=30,\n",
    "                                      n_hinges=30,\n",
    "                                      n_rbf=40,\n",
    "                                      n_noise=120,\n",
    "                                      n_x_noise=40,\n",
    "                                      n_dupes=20,\n",
    "                                      seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b554fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_overfit.shape)\n",
    "X_overfit[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba15b1-3f43-45f5-8600-8eec0b80a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_overfit, prices, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = model.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "r2_train = r2_score(y_train, y_hat_train)\n",
    "\n",
    "y_hat_test = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_hat_test)\n",
    "r2 = r2_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246558ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sorted = X_train[X_train[:, 0].argsort()]\n",
    "y_hat_train_sorted = model.predict(X_train_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[:, 0], y_train, alpha=0.5)\n",
    "plt.plot(X_train_sorted[:, 0], y_hat_train_sorted, color='red', linewidth=2)\n",
    "plt.title(f'Linear Regression - performance on Training data\\nMSE: {mse_train:,.0f}\\nR squared: {r2_train*100:.0f}%')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd67a3c-141e-4365-9f1c-d126decaeeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort X_test for smooth curve plotting\n",
    "\n",
    "X_test_sorted = X_test[X_test[:, 0].argsort()]\n",
    "y_hat_test_sorted = model.predict(X_test_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892a4c4-289e-465b-96c7-ceb87766d709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test[:, 0], y_test, alpha=0.5)\n",
    "plt.plot(X_test_sorted[:, 0], y_hat_test_sorted, color='red', linewidth=2)\n",
    "plt.title(f'Linear Regression - performance on unseen Test data\\nMSE: {mse:,.0f}\\nR squared: {r2*100:.0f}%')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4785e-070b-46d0-9930-fea0485a6bdb",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "And so you see the surprising results. We provided the Linear Regression with the same festures as before, and MORE features if it wanted them.\n",
    "\n",
    "And something went wrong: we got worse results.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because armed with the extra flexibility, the model was able to fit more closely to the training data.\n",
    "\n",
    "But it was finding new patterns in the noise that DIDN'T REFLECT the underlying pattern.\n",
    "\n",
    "So when we extended it to make new predictions, it did worse.\n",
    "\n",
    "And that is over-fitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14f7b9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

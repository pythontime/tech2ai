{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49a602f-1d11-4369-9164-f5364296bcf0",
   "metadata": {},
   "source": [
    "# Segment 3 Extra Lab\n",
    "\n",
    "## Let's make a deeper neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8d95d-8596-481c-9d4b-61cc810c65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports - now including pytorch\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from evaluator import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d8ba4-7e34-41ea-87f3-92865067b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset\n",
    "# Sidenote: this is actually a larger dataset than before (about twice as large)\n",
    "\n",
    "with open('../train.pkl', 'rb') as file:\n",
    "    train = pickle.load(file)\n",
    "\n",
    "with open('../test.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e233e-2c41-4a9a-99e0-c81c07597e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dec838-1230-42dd-8626-4b4929013319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "DB = \"../segment4/products_vectorstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd66f7-25ab-4454-b241-e0738becb941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "# If you don't have a HuggingFace account, you can set one up for free at www.huggingface.co\n",
    "# And then add the HF_TOKEN to your .env file as explained in the project README\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(token=hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ebe98-73c1-4eae-9b65-275e466071d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=DB)\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8c8ab-5de1-45d2-8eee-7dca95f2959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"products\"\n",
    "collection = client.get_or_create_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6a97a-c812-4100-8076-feaf6005ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "prices = [metadata['price'] for metadata in result['metadatas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1a300-da16-4324-a435-2aa3f9678422",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(vectors)\n",
    "y_train_tensor = torch.FloatTensor(prices).unsqueeze(1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tensor, y_train_tensor, test_size=0.01, random_state=42)\n",
    "\n",
    "# Log\n",
    "y_train_log = torch.log(y_train + 1)\n",
    "y_val_log = torch.log(y_val + 1)\n",
    "    \n",
    "# Normalize log prices\n",
    "y_mean = y_train_log.mean()\n",
    "y_std = y_train_log.std()\n",
    "y_train_norm = (y_train_log - y_mean) / y_std\n",
    "y_val_norm = (y_val_log - y_mean) / y_std\n",
    "\n",
    "# Create the loader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824452be-3387-403e-8e4f-f8ab0fc99700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_layers=10, hidden_size=4096, dropout_prob=0.2):\n",
    "        super(NewNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # First layer\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        for i in range(num_layers - 2):\n",
    "            self.residual_blocks.append(\n",
    "                ResidualBlock(hidden_size, dropout_prob)\n",
    "            )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49dec4-7115-4fb6-9f14-4748d38f9ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_prob):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual  # Skip connection\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a2977-5764-4a40-a65b-c746ce31b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NewNeuralNetwork(X_train.shape[1])\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train_norm)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfec868-fb13-4fd7-9201-b62b10b94885",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_START = 1\n",
    "EPOCH_END = 5\n",
    "\n",
    "for epoch in range(EPOCH_START, EPOCH_END+1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for batch_X, batch_y in tqdm(train_loader):\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val.to(device))\n",
    "        val_loss = loss_function(val_outputs, y_val_norm.to(device))\n",
    "        \n",
    "        # Convert back to original scale for meaningful metrics\n",
    "        val_outputs_orig = torch.exp(val_outputs * y_std + y_mean) - 1\n",
    "        mae = torch.abs(val_outputs_orig - y_val.to(device)).mean()\n",
    "    \n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCH_END}]')\n",
    "    print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "    print(f'Val MAE (original scale): ${mae.item():.2f}')\n",
    "    print(f'Learning rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "\n",
    "    # torch.save(model.state_dict(), f'models/nnn-{epoch+1}.pth')\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39515e66-4d4b-4123-8489-575b02e56c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def new_neural_network(item):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vector = encoder.encode(item.text)\n",
    "        vector = torch.FloatTensor(vector).to(device)\n",
    "        pred = model(vector)[0]\n",
    "        result = torch.exp(pred * y_std + y_mean) - 1\n",
    "        result = result.item()\n",
    "    return max(0, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9117e-540f-400a-af2d-c17c24029b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_neural_network(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f899c0c-87a5-41cb-a2f3-6c4fe9498cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(new_neural_network, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63561f9a-fc46-4e8b-ad7d-44ae85219484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
